Using Theano backend.
Fold 1/8
Train on 18001 samples, validate on 4423 samples
Epoch 1/50
18001/18001 [==============================] - 241s - loss: 2.0312 - acc: 0.3340 - val_loss: 1.8009 - val_acc: 0.3920
Epoch 2/50
18001/18001 [==============================] - 237s - loss: 1.0762 - acc: 0.6283 - val_loss: 2.0973 - val_acc: 0.4404
Epoch 3/50
18001/18001 [==============================] - 242s - loss: 0.6893 - acc: 0.7666 - val_loss: 2.0128 - val_acc: 0.4798
Epoch 4/50
17984/18001 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8353Epoch 00003: early stopping
18001/18001 [==============================] - 244s - loss: 0.5078 - acc: 0.8352 - val_loss: 2.2245 - val_acc: 0.4705
4423/4423 [==============================] - 14s
Score: 2.2244666585
79726/79726 [==============================] - 271s
Fold 2/8
Train on 16538 samples, validate on 5886 samples
Epoch 1/50
16538/16538 [==============================] - 228s - loss: 1.9706 - acc: 0.3516 - val_loss: 1.7394 - val_acc: 0.3323
Epoch 2/50
16538/16538 [==============================] - 227s - loss: 1.0750 - acc: 0.6331 - val_loss: 1.6725 - val_acc: 0.4358
Epoch 3/50
16538/16538 [==============================] - 224s - loss: 0.7188 - acc: 0.7666 - val_loss: 1.6803 - val_acc: 0.4545
Epoch 4/50
16538/16538 [==============================] - 224s - loss: 0.5327 - acc: 0.8297 - val_loss: 1.8222 - val_acc: 0.4589
Epoch 5/50
16512/16538 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8660Epoch 00004: early stopping
16538/16538 [==============================] - 226s - loss: 0.4227 - acc: 0.8660 - val_loss: 1.7459 - val_acc: 0.5117
5886/5886 [==============================] - 19s
Score: 1.74589958223
79726/79726 [==============================] - 268s
Fold 3/8
Train on 17162 samples, validate on 5262 samples
Epoch 1/50
17162/17162 [==============================] - 234s - loss: 2.0083 - acc: 0.3363 - val_loss: 1.7111 - val_acc: 0.3675
Epoch 2/50
17162/17162 [==============================] - 231s - loss: 1.1046 - acc: 0.6217 - val_loss: 1.5744 - val_acc: 0.4371
Epoch 3/50
17162/17162 [==============================] - 231s - loss: 0.7170 - acc: 0.7655 - val_loss: 1.4981 - val_acc: 0.4780
Epoch 4/50
17162/17162 [==============================] - 235s - loss: 0.5270 - acc: 0.8295 - val_loss: 1.4578 - val_acc: 0.5144
Epoch 5/50
17162/17162 [==============================] - 238s - loss: 0.4199 - acc: 0.8688 - val_loss: 1.3126 - val_acc: 0.5711
Epoch 6/50
17162/17162 [==============================] - 236s - loss: 0.3432 - acc: 0.8927 - val_loss: 1.3692 - val_acc: 0.5785
Epoch 7/50
17162/17162 [==============================] - 235s - loss: 0.2804 - acc: 0.9118 - val_loss: 1.3439 - val_acc: 0.5895
Epoch 8/50
17152/17162 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9218Epoch 00007: early stopping
17162/17162 [==============================] - 238s - loss: 0.2515 - acc: 0.9217 - val_loss: 1.3570 - val_acc: 0.5868
5262/5262 [==============================] - 18s
Score: 1.35702488064
79726/79726 [==============================] - 274s
Fold 4/8
Train on 16778 samples, validate on 5646 samples
Epoch 1/50
16778/16778 [==============================] - 233s - loss: 2.0833 - acc: 0.3229 - val_loss: 1.5573 - val_acc: 0.5197
Epoch 2/50
16778/16778 [==============================] - 235s - loss: 1.2094 - acc: 0.5773 - val_loss: 1.3494 - val_acc: 0.5939
Epoch 3/50
16778/16778 [==============================] - 234s - loss: 0.8199 - acc: 0.7267 - val_loss: 1.2021 - val_acc: 0.6238
Epoch 4/50
16778/16778 [==============================] - 235s - loss: 0.5854 - acc: 0.8106 - val_loss: 1.2300 - val_acc: 0.6428
Epoch 5/50
16778/16778 [==============================] - 235s - loss: 0.4595 - acc: 0.8506 - val_loss: 1.2765 - val_acc: 0.6401
Epoch 6/50
16768/16778 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8784Epoch 00005: early stopping
16778/16778 [==============================] - 234s - loss: 0.3863 - acc: 0.8784 - val_loss: 1.3883 - val_acc: 0.6123
5646/5646 [==============================] - 19s
Score: 1.3892393391
79726/79726 [==============================] - 278s
Fold 5/8
Train on 16355 samples, validate on 6069 samples
Epoch 1/50
16355/16355 [==============================] - 229s - loss: 2.1026 - acc: 0.3080 - val_loss: 1.6688 - val_acc: 0.4376
Epoch 2/50
16355/16355 [==============================] - 232s - loss: 1.1371 - acc: 0.6004 - val_loss: 1.6063 - val_acc: 0.4759
Epoch 3/50
16355/16355 [==============================] - 230s - loss: 0.7436 - acc: 0.7549 - val_loss: 1.6398 - val_acc: 0.4754
Epoch 4/50
16355/16355 [==============================] - 230s - loss: 0.5482 - acc: 0.8208 - val_loss: 1.9559 - val_acc: 0.4765
Epoch 5/50
16320/16355 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.8624Epoch 00004: early stopping
16355/16355 [==============================] - 231s - loss: 0.4279 - acc: 0.8626 - val_loss: 2.0537 - val_acc: 0.4708
6069/6069 [==============================] - 21s
Score: 2.05420942809
79726/79726 [==============================] - 279s
Fold 6/8
Train on 16734 samples, validate on 5690 samples
Epoch 1/50
16734/16734 [==============================] - 234s - loss: 2.1576 - acc: 0.2980 - val_loss: 1.7185 - val_acc: 0.4125
Epoch 2/50
16734/16734 [==============================] - 232s - loss: 1.1828 - acc: 0.5894 - val_loss: 1.6074 - val_acc: 0.4571
Epoch 3/50
16734/16734 [==============================] - 230s - loss: 0.7711 - acc: 0.7436 - val_loss: 1.4549 - val_acc: 0.4988
Epoch 4/50
16734/16734 [==============================] - 232s - loss: 0.5823 - acc: 0.8103 - val_loss: 1.4252 - val_acc: 0.5605
Epoch 5/50
16734/16734 [==============================] - 227s - loss: 0.4621 - acc: 0.8508 - val_loss: 1.4628 - val_acc: 0.5566
Epoch 6/50
16734/16734 [==============================] - 227s - loss: 0.3712 - acc: 0.8817 - val_loss: 1.5954 - val_acc: 0.5636
Epoch 7/50
16704/16734 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.8995Epoch 00006: early stopping
16734/16734 [==============================] - 226s - loss: 0.3122 - acc: 0.8996 - val_loss: 1.5733 - val_acc: 0.5692
5690/5690 [==============================] - 19s
Score: 1.57331803124
79726/79726 [==============================] - 265s
Fold 7/8
Train on 16759 samples, validate on 5665 samples
Epoch 1/50
16759/16759 [==============================] - 226s - loss: 2.1838 - acc: 0.2809 - val_loss: 1.6112 - val_acc: 0.4533
Epoch 2/50
16759/16759 [==============================] - 227s - loss: 1.2074 - acc: 0.5753 - val_loss: 1.3941 - val_acc: 0.5206
Epoch 3/50
16759/16759 [==============================] - 226s - loss: 0.7970 - acc: 0.7305 - val_loss: 1.2400 - val_acc: 0.5931
Epoch 4/50
16759/16759 [==============================] - 226s - loss: 0.5990 - acc: 0.8039 - val_loss: 1.2985 - val_acc: 0.5986
Epoch 5/50
16759/16759 [==============================] - 226s - loss: 0.4659 - acc: 0.8496 - val_loss: 1.2554 - val_acc: 0.5862
Epoch 6/50
16704/16759 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8780Epoch 00005: early stopping
16759/16759 [==============================] - 226s - loss: 0.3861 - acc: 0.8782 - val_loss: 1.2675 - val_acc: 0.6222
5665/5665 [==============================] - 18s
Score: 1.27008684013
79726/79726 [==============================] - 265s
Fold 8/8
Train on 16314 samples, validate on 6110 samples
Epoch 1/50
16314/16314 [==============================] - 224s - loss: 2.0345 - acc: 0.3259 - val_loss: 1.7275 - val_acc: 0.4450
Epoch 2/50
16314/16314 [==============================] - 222s - loss: 1.1427 - acc: 0.6048 - val_loss: 1.7600 - val_acc: 0.5144
Epoch 3/50
16314/16314 [==============================] - 221s - loss: 0.7442 - acc: 0.7554 - val_loss: 2.0297 - val_acc: 0.5123
Epoch 4/50
16256/16314 [============================>.] - ETA: 0s - loss: 0.5379 - acc: 0.8281Epoch 00003: early stopping
16314/16314 [==============================] - 222s - loss: 0.5379 - acc: 0.8282 - val_loss: 2.1665 - val_acc: 0.5196
6110/6110 [==============================] - 20s
Score: 2.1665394378
79726/79726 [==============================] - 265s
Writing submission for 8 folds, score: 1.68542298476...
Done.