{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> Load pretrained model...\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Load data...\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: colorspace RGB -> YUV\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: normalize each feature (channel) globally\t\n",
       "1\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: normalize all three channels locally\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> target:\t\n",
       "2\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> prediction:\t\n",
       "2 - 96.200634953783%\t\n",
       "7 - 3.2933205198945%\t\n",
       "8 - 0.22320056510039%\t\n",
       "1 - 0.18136812230258%\t\n",
       "3 - 0.047649911536933%\t\n",
       "4 - 0.025340763976744%\t\n",
       "9 - 0.0076422435460202%\t\n",
       "6 - 0.0065928905305181%\t\n",
       "10 - 0.0046131396081926%\t\n",
       "5 - 0.00038118245220448%\t\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 34,
       "width": 34
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "require \"torch\"\n",
    "require \"cunn\"\n",
    "require \"nn\"\n",
    "require \"image\"\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "cmd = torch.CmdLine()\n",
    "cmd:text('Options:')\n",
    "cmd:option('-type', 'double', 'type: double | float | cuda')\n",
    "cmd:text()\n",
    "opt = cmd:parse(arg or {})\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "if opt.type == 'float' then\n",
    "   print('==> switching to floats')\n",
    "   torch.setdefaulttensortype('torch.FloatTensor')\n",
    "elseif opt.type == 'cuda' then\n",
    "   print('==> switching to CUDA')\n",
    "   require 'cunn'\n",
    "   torch.setdefaulttensortype('torch.FloatTensor')\n",
    "end\n",
    "\n",
    "print(\"==> Load pretrained model...\")\n",
    "model = torch.load(\"results/model.net\")\n",
    "\n",
    "-- print(\"==> Pretrained model loaded: \")\n",
    "-- print(model)\n",
    "\n",
    "print(\"==> Load data...\")\n",
    "test_file = 'test_32x32.t7'\n",
    "tesize = 2000 -- 26032\n",
    "loaded = torch.load(test_file,'ascii')\n",
    "testData = {\n",
    "   data = loaded.X:transpose(3,4),\n",
    "   labels = loaded.y[1],\n",
    "   size = function() return tesize end\n",
    "}\n",
    "testData.data = testData.data:double()\n",
    "\n",
    "-- Convert all images to YUV\n",
    "print '==> preprocessing data: colorspace RGB -> YUV'\n",
    "for i = 1,testData:size() do\n",
    "   testData.data[i] = image.rgb2yuv(testData.data[i])\n",
    "end\n",
    "\n",
    "-- Name channels for convenience\n",
    "channels = {'y','u','v'}\n",
    "\n",
    "-- Normalize each channel, and store mean/std\n",
    "-- per channel. These values are important, as they are part of\n",
    "-- the trainable parameters. At test time, test data will be normalized\n",
    "-- using these values.\n",
    "print '==> preprocessing data: normalize each feature (channel) globally'\n",
    "mean = {\n",
    "    -0.006313964130324,\n",
    "    0.21264134455774,\n",
    "    0.22919727583664\n",
    "}\n",
    "std = {\n",
    "    0.94085703702485,\n",
    "    0.78796429042957,\n",
    "    0.74868942342819\n",
    "}\n",
    "\n",
    "-- Normalize test data, using the training means/stds\n",
    "for i,channel in ipairs(channels) do\n",
    "    print(i)\n",
    "   -- normalize each channel globally:\n",
    "   testData.data[{ {},i,{},{} }]:add(-mean[i])\n",
    "   testData.data[{ {},i,{},{} }]:div(std[i])\n",
    "end\n",
    "\n",
    "-- Local normalization\n",
    "print '==> preprocessing data: normalize all three channels locally'\n",
    "\n",
    "-- Define the normalization neighborhood:\n",
    "neighborhood = image.gaussian1D(13)\n",
    "\n",
    "-- Define our local normalization operator (It is an actual nn module, \n",
    "-- which could be inserted into a trainable model):\n",
    "normalization = nn.SpatialContrastiveNormalization(1, neighborhood, 1):double()\n",
    "\n",
    "-- Normalize all channels locally:\n",
    "for c in ipairs(channels) do\n",
    "   for i = 1,testData:size() do\n",
    "      testData.data[{ i,{c},{},{} }] = normalization:forward(testData.data[{ i,{c},{},{} }])\n",
    "   end\n",
    "end\n",
    "\n",
    "\n",
    "model:evaluate()\n",
    "\n",
    "-- get new sample\n",
    "local t = 888\n",
    "local input = testData.data[t]\n",
    "local target = testData.labels[t]\n",
    "print(\"==> target:\")\n",
    "print(target)\n",
    "\n",
    "-- test sample\n",
    "local prediction = model:forward(input):exp()\n",
    "local confidences, indices = torch.sort(prediction, true)\n",
    "print(\"==> prediction:\")\n",
    "for i=1,confidences:size(1) do\n",
    "    print(indices[i] .. \" - \" .. confidences[i] * 100 .. \"%\")\n",
    "end\n",
    "\n",
    "if itorch then\n",
    "    itorch.image(loaded.X[{ {t,t} }])\n",
    "else\n",
    "    print('skipping visualization because the script is not run from itorch')\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
